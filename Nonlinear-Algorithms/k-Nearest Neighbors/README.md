# k-Nearest Neighbors

<img src='0417red_WinInsider.jpeg' align='right' width=450/>

In statistics, the k-nearest neighbors algorithm (k-NN) is a non-parametric supervised learning method first developed by Evelyn Fix and Joseph Hodges in 1951,[1] and later expanded by Thomas Cover.[2] It is used for classification and regression. In both cases, the input consists of the k closest training examples in a dataset[3].

$$Euclidean Distance (a,b) = \sum_{i=1}^{n}(a_i - b_I)^2$$

---
## References

[1] [Discriminatory Analysis. Nonparametric Discrimination: Consistency Properties](https://apps.dtic.mil/dtic/tr/fulltext/u2/a800276.pdf)

[2] [Nearest neighbor pattern classification](http://ssg.mit.edu/cal/abs/2000_spring/np_dens/classification/cover67.pdf)

[3] [*k*-nearest neighbors algorithm](https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm)
