# Master Machine Learning Algorithms ðŸ“•

This is to practice the machine learning algorithms mentioned in the book [Master Machine Learning Algorithms](https://machinelearningmastery.com/master-machine-learning-algorithms/) by Jason Brownlee. I'll first manually implement the algorithms using a generated sample data based on the tutorials in the book and excel sheets provided by the book, then apply the related function from `sklearn` library, and finally apply that function to a previously collected dataset. The organization of this repository as follows:  

* [Linear Algorithms](https://github.com/muscak/Master-Machine-Learning-Algorithms/tree/master/Linear-Algorithms)
  * [Linear Regression](https://github.com/muscak/Master-Machine-Learning-Algorithms/tree/master/Linear-Algorithms/Linear-Regression)
  * [Logistic Regression](https://github.com/muscak/Master-Machine-Learning-Algorithms/tree/master/Linear-Algorithms/Logistic-Regression)
  * [Linear Discriminant Analysis (LDA)](https://github.com/muscak/Master-Machine-Learning-Algorithms/tree/master/Linear-Algorithms/LDA)
* [Nonlinear Algorithms](https://github.com/muscak/Master-Machine-Learning-Algorithms/tree/master/Nonlinear-Algorithms)
  * [Classification and Regression Trees (CART)](https://github.com/muscak/Master-Machine-Learning-Algorithms/tree/master/Nonlinear-Algorithms/CART)
  * [Naive Bayes](https://github.com/muscak/Master-Machine-Learning-Algorithms/tree/master/Nonlinear-Algorithms/Naive-Bayes)
  * [Gaussian Naive Bayes (GNB)](https://github.com/muscak/Master-Machine-Learning-Algorithms/tree/master/Nonlinear-Algorithms/Gaussian-Naive-Bayes)
  * [$k$-Nearest Neighbors (*k*NN)](https://github.com/muscak/Master-Machine-Learning-Algorithms/tree/master/Nonlinear-Algorithms/k-Nearest%20Neighbors)
  * [Learning Vector Quantization (LVQ)](https://github.com/muscak/Master-Machine-Learning-Algorithms/tree/master/Nonlinear-Algorithms/Learning-Vector-Quantization)
  * [Support Vector Machines (SVM)](https://github.com/muscak/Master-Machine-Learning-Algorithms/tree/master/Nonlinear-Algorithms/Support-Vector-Machines)
* [Ensemble Algorithms](https://github.com/muscak/Master-Machine-Learning-Algorithms/tree/master/Ensemble-Algorithms)
  * [Bagging and Random Forest](https://github.com/muscak/Master-Machine-Learning-Algorithms/tree/master/Ensemble-Algorithms/Bagging)
  * [Boosting and AdaBoost](https://github.com/muscak/Master-Machine-Learning-Algorithms/tree/master/Ensemble-Algorithms/Boosting)
  
These algorithms can be group in a different way as well. Linear Regression, Logistic Regression, and SVMs are methods where the form of the model was **pre-defined**. On the other hand, $k$-Nearest Neighbours and Decision Trees are **non-parametric** learners which do not have a model structure specified a priori [1].

---

[1] [Machine Learning for Humans](https://everythingcomputerscience.com/books/Machine%20Learning%20for%20Humans.pdf)
